{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft margin SVM\n",
    "Soft margin SVM is a branch of SVM (Support Vector Machines) that allows the model to make some level of misclassifications as to make the decision boundary (SOFTER)\n",
    "\n",
    "Specifically, it aims to solve the following dual problem \n",
    "\n",
    "$$\n",
    "max \\space \\sum_{i}\\alpha_i - \\frac{1}{2}\\sum_i \\sum_j y^{(i)}y^{(j)}a_ia_j<x^{(i)}, x^{(j)}> \\\\\n",
    "s.t. \\space C \\ge \\alpha_i \\ge 0 , \\sum_i y^{(i)}\\alpha_i = 0\n",
    "$$\n",
    "\n",
    "With the following KKT conditions\n",
    "\n",
    "$$\n",
    "a_i = 0 \\Rightarrow y^{(i)}(w^Tx^{(i)}+b) \\ge 1 \\\\ \n",
    "a_i = C \\Rightarrow y^{(i)}(w^Tx^{(i)}+b) \\le 1 \\\\ \n",
    "C \\ge a_i \\ge 0 \\Rightarrow y^{(i)}(w^Tx^{(i)}+b) = 1\n",
    "$$\n",
    "\n",
    "Along side with kernel trick, SMO is one of the powerful tools that can do so. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Values\n",
    "- **point** corresponds to the training data $x_i$\n",
    "- **target** corresponds to the training outputs $y_i$\n",
    "- **C** is inversely proportional to the amount of mistakes we can afford. This depends on the scale of the problem. Mostly itâ€™s set from $.01 \\to100$\n",
    "- **tol** is the amount of tolerance we will have for the KKT conditions.\n",
    "- **prog_margin** is the padding we will employ for the calculation of $L$ and $H$ as to not make them equal. This will also serve as our margin in determining whether the two langrange multiplier has made any positive progress.\n",
    "- **clip_padding** is the padding we will apply on the constraint $C \\ge a_i \\ge 0$ where we wil clip $a_i$ to either $C$ or $0$ if itâ€™s within that padding\n",
    "\n",
    "â˜ðŸ» tol, prog_margin and clip_padding are mostly set to $1e^-3$ to $1e^-5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(690420)\n",
    "\n",
    "M: int = 100\n",
    "D: int = 1\n",
    "point = np.random.normal(size=(M, D), loc=0, scale=10).astype(np.float64)\n",
    "target = np.random.choice([-1, 1], size=(M), replace=True)\n",
    "c: np.float64 = 5\n",
    "tol: np.float64 = 0.0001\n",
    "prog_margin: np.float64 = 0.00001\n",
    "clip_padding: np.float64 = 0.000000001\n",
    "max_ch: int = 600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Function\n",
    "Function responsible for the kernel trick\n",
    "\n",
    "For gaussian Kernels we use the following \n",
    "$$\n",
    "K(x ,z ) = exp(-\\frac{||x-z||^2}{2\\sigma})\n",
    "$$\n",
    "\n",
    "This can be sped up from the fact that $||x-z||^2 = x * x - 2x*z + z *z $\n",
    "\n",
    "We can cache the dot product of vector to itself. We can also store the dot product of every 2 possible pair! but this may take a lot of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_gaussian(x1: np.float64, x2: np.float64, sigma: np.float64 = 1):\n",
    "        if np.ndim(x1) == 1 and np.ndim(x2) == 1:\n",
    "            return np.exp(-(np.linalg.norm(x1-x2,2))**2/(2*sigma**2))\n",
    "        elif(np.ndim(x1)>1 and np.ndim(x2) == 1) or (np.ndim(x1) == 1 and np.ndim(x2)>1):\n",
    "            return np.exp(-(np.linalg.norm(x1-x2, 2, axis=1)**2)/(2*sigma**2))\n",
    "        elif np.ndim(x1) > 1 and np.ndim(x2) > 1 :\n",
    "            return np.exp(-(np.linalg.norm(x1[:, np.newaxis] \\\n",
    "                             - x2[np.newaxis, :], 2, axis = 2) ** 2)/(2*sigma**2))\n",
    "        \n",
    "        return 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_svm_err(x: int, alphs: np.float64, err_cache: np.float64, b: np.float64) -> np.float64:\n",
    "    fx: np.float64 = obj_x(point[x], alphs, b,)\n",
    "    err_cache[x] = fx - target[x]\n",
    "    return err_cache[x]     \n",
    "\n",
    "def obj_x(val: np.float64, alphs: np.float64, b: np.float64) -> np.float64:\n",
    "    return (alphs * target) @ kernel_gaussian(point, val) + b\n",
    "\n",
    "def accuracy(alphs: np.float64, b: np.float64) -> np.float64:\n",
    "    correct: float = 0.0\n",
    "\n",
    "    for i in range(M):\n",
    "        fx: np.float64 = obj_x(point[i], alphs, b)\n",
    "        correct += (target[i]*fx > 0)\n",
    "    \n",
    "    return correct / M\n",
    "\n",
    "def predict(alphs: np.float64, b: np.float64, x: np.float64, new: bool = False) -> np.float64:\n",
    "    res: np.float64 = np.zeros(shape=(len(x)), dtype=np.float64)\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        fx: np.float64 = obj_x(x[i], alphs, b)\n",
    "        res[i] = fx\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Function\n",
    "Lastly the function that takes a coordinate ascent step given the two multipliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_step(i1: int, i2: int, alphs: np.float64, non_bound_alphs: np.int64, err_cache: np.float64, B: np.float64, log: bool=False) -> bool:\n",
    "    #print(f\"Taking step for {i1} and {i2}\")\n",
    "    if (i1 == i2):\n",
    "        #print(\"Positions equal\")\n",
    "        return 0\n",
    "    \n",
    "    non_bound: bool = alphs[i1] != 0 and alphs[i1] != c\n",
    "    b: np.float64 = B[0]\n",
    "\n",
    "    E1: np.float64 = err_cache[i1] if non_bound else compute_svm_err(i1, alphs, err_cache, b)\n",
    "    E2: np.float64 = err_cache[i2] \n",
    "    y1: int = target[i1]\n",
    "    y2: int = target[i2]\n",
    "    alph1: np.float64 = alphs[i1]\n",
    "    alph2: np.float64 = alphs[i2]\n",
    "    s: int = y1 * y2\n",
    "\n",
    "    # Computation for L and H\n",
    "    if (y1 == y2):\n",
    "        L: np.float64 = max(0, alph1 + alph2 - c)\n",
    "        H: np.float64 = min(c, alph1 + alph2)\n",
    "    else:\n",
    "        L: np.float64 = max(0, alph2 - alph1)\n",
    "        H: np.float64 = min(c, c + alph2 - alph1)\n",
    "\n",
    "    if (L == H):\n",
    "        #print(\"L and H equal\")\n",
    "        return 0\n",
    "    \n",
    "    K11: np.float64 = kernel_gaussian(point[i1], point[i1])\n",
    "    K22: np.float64 = kernel_gaussian(point[i2], point[i2])\n",
    "    K12: np.float64 = kernel_gaussian(point[i1], point[i2])\n",
    "    eta: np.float64 = K11 + K22 - 2*K12\n",
    "\n",
    "    if (eta > 0):\n",
    "        alph2_new: np.float64 = (alph2) + (y2*(E1 - E2)/eta)\n",
    "        if (alph2_new <= L):\n",
    "            alph2_new = L\n",
    "        elif (alph2_new >= H):\n",
    "            alph2_new = H\n",
    "    else:\n",
    "        v1: np.float64 = E1 - alph1*y1*K11 - alph2*y2*K12\n",
    "        v2: np.float64 = E2 - alph1*y1*K12 - alph2*y2*K22\n",
    "        zeta: np.float64 = alph1*y1 + alph2*y2\n",
    "        Lobj: np.float64 = L*(1-s) + zeta*s*L*K11 - (.05*(L**2)*(K11 + K22)) - (zeta - s*L)*s*L*K12 + (v1 - v2)*y2*L\n",
    "        Hobj: np.float64 = H*(1-s) + zeta*s*H*K11 - (.05*(H**2)*(K11 + K22)) - (zeta - s*H)*s*H*K12 + (v1 - v2)*y2*H\n",
    "\n",
    "        if (Lobj < Hobj - prog_margin):\n",
    "            alph2_new: np.float64 = L\n",
    "        elif (Lobj > Hobj + prog_margin):\n",
    "            alph2_new: np.float64 = H    \n",
    "        else:\n",
    "            alph2_new: np.float64 = alph2\n",
    "\n",
    "    if (abs(alph2_new - alph2) < (prog_margin * (alph2_new + alph2 + prog_margin))):\n",
    "        #print(\"Bad progress\")\n",
    "        return 0\n",
    "    \n",
    "    alph1_new: np.float64 = alph1 + (s*(alph2 - alph2_new))\n",
    "\n",
    "    # clip\n",
    "    if (alph1_new < clip_padding):\n",
    "        alph1_new = 0\n",
    "    elif (alph1_new > c - clip_padding):\n",
    "        alph1_new = c\n",
    "        \n",
    "    # update tresholds\n",
    "    b1: np.float64 = b - E1 - y1*K11*(alph1_new - alph1) - y2*K12*(alph2_new - alph2)\n",
    "    b2: np.float64 = b - E2 - y1*K12*(alph1_new - alph1) - y2*K22*(alph2_new - alph2)\n",
    "    \n",
    "    if (alph1_new < c and alph1_new > 0):\n",
    "        B[0] = b1\n",
    "    elif (alph2_new < c and alph2_new > 0):\n",
    "        B[0] = b2\n",
    "    else:\n",
    "        B[0] = (b1 + b2)/2\n",
    "\n",
    "    # update alphs\n",
    "    alphs[i1], alphs[i2] = alph1_new, alph2_new\n",
    "\n",
    "    # update err_cache\n",
    "    err_cache[i1], err_cache[i2] = 0, 0\n",
    "    for i in non_bound_alphs:\n",
    "        if (i == i1 or i == i2):\n",
    "            continue\n",
    "        \n",
    "        K1k: np.float64 = kernel_gaussian(point[i1], point[i])\n",
    "        K2k: np.float64 = kernel_gaussian(point[i2], point[i])\n",
    "\n",
    "        err_cache[i] += y1*K1k*(alph1_new - alph1) + y2*K2k*(alph2_new - alph2) + B[0] - b\n",
    "    \n",
    "    if (log):\n",
    "        print(f\"======= Step successful for {i1} and {i2} =======\")\n",
    "        print(f\"a1: {alph1} -> {alph1_new} | a2: {alph2} -> {alph2_new}\")\n",
    "        print(f\"b: {b} -> {B[0]}\")\n",
    "        print(f\"err_cache: {err_cache}\")\n",
    "        print(\"==================================================\")\n",
    "\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Alpha Function\n",
    "The second function which is responsible for checking the first langrange multiplier that is chosen and responsible for picking the next langrange multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_a(i2: int, alphs: np.array, non_bound: bool, non_bound_alphs: np.int64, err_cache: np.array, B: np.float64) -> bool:\n",
    "    non_b_len: int = len(non_bound_alphs)\n",
    "    b: np.float64 = B[0]\n",
    "    E2: np.float64 = err_cache[i2] if non_bound else compute_svm_err(i2, alphs, err_cache, b)\n",
    "    r2: np.float64 = E2 * target[i2]\n",
    "    alph2: np.float64 = alphs[i2]\n",
    "\n",
    "    if ((r2 < -tol and alph2 < c) or (r2 > tol and alph2 > 0)):\n",
    "        #print(\"choosing second multipier\")\n",
    "        if (non_b_len > 1):\n",
    "            # Second heuristic using optimal err for step estimation\n",
    "            positive: bool = (err_cache[i2] >= 0)\n",
    "            i1: int = i2\n",
    "\n",
    "            for i in non_bound_alphs:\n",
    "                if (i == i2):\n",
    "                    continue\n",
    "\n",
    "                if (i1 == i2):\n",
    "                    i1 = i\n",
    "                elif (positive and err_cache[i] < err_cache[i1]):\n",
    "                    i1 = i\n",
    "                elif (not positive and err_cache[i] > err_cache[i1]):\n",
    "                    i1 = i\n",
    "\n",
    "            if (take_step(i1, i2, alphs, non_bound_alphs, err_cache, B)):\n",
    "                return 1\n",
    "\n",
    "        # take non-bound alps\n",
    "        if non_b_len > 0:\n",
    "            start: int = np.random.randint(size=(1), low=0, high=non_b_len)[0]\n",
    "            for i in range(non_b_len):\n",
    "                pos: int = (start + i)%non_b_len\n",
    "                if (take_step(non_bound_alphs[pos], i2, alphs, non_bound_alphs, err_cache, B)):\n",
    "                    return 1\n",
    "        \n",
    "        # loop through entire training set\n",
    "        start: int = np.random.randint(size=(1), low=0, high=M)[0]\n",
    "        for i in range(M):\n",
    "            if (take_step((start + i)%M, i2, alphs, non_bound_alphs, err_cache, B)):\n",
    "                return 1\n",
    "    \n",
    "    #print(\"already satisfy kkt conditions\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Function\n",
    "The train function which is responsible for picking the first langrange multiplier from a set of langrange multipliers. It is also responsible for initializing the important variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smo_train() -> dict:\n",
    "    alphs: np.float64 = np.zeros(shape=(M), dtype=np.float64)\n",
    "    err_cache: np.float64 = np.zeros(shape=(M), dtype=np.float64)\n",
    "    B: np.float64 = np.array([0.], dtype=np.float64)\n",
    "\n",
    "    examine_all: bool = True\n",
    "    num_changed: int = 0\n",
    "    total_iter: int = 0\n",
    "\n",
    "    while num_changed > 0 or examine_all:\n",
    "        #print(\"choosing first multiplier\")\n",
    "        if total_iter >= max_ch:\n",
    "            break\n",
    "\n",
    "        num_changed = 0 \n",
    "        non_bound_alphs = np.where((alphs != 0) & (alphs != c))[0]\n",
    "\n",
    "        if examine_all:\n",
    "            for i in range(M):\n",
    "                num_changed += examine_a(i, alphs, False, non_bound_alphs, err_cache, B)\n",
    "        else:\n",
    "            for i in non_bound_alphs:\n",
    "                num_changed += examine_a(i, alphs, True, non_bound_alphs, err_cache, B)\n",
    "\n",
    "        if examine_all:\n",
    "            examine_all = False\n",
    "        elif num_changed == 0:\n",
    "            examine_all = True\n",
    "\n",
    "        total_iter += 1\n",
    "        #print(\"Total iterations: \", total_iter)\n",
    "\n",
    "    #print(err_cache)\n",
    "    return {\"alphs\": alphs, \"err_cache\": err_cache, \"b\": B[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "You can test the result here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nkernel_g_cache = initialize_kernels(M) \\nres = smo_train()\\nprint(accuracy(res['alphs'], res['b']))\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "'''\n",
    "kernel_g_cache = initialize_kernels(M) \n",
    "res = smo_train()\n",
    "print(accuracy(res['alphs'], res['b']))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEMO\n",
    "This Demo only works in 2 Dimensions, hence the number of parameters are at most 2\n",
    "Also, this overwrites the global variables above because the format of the code above is not object oriented (for note taking purposes)\n",
    "\n",
    "n = number of points per classification (i.e. M/2)\n",
    "\n",
    "Change n to change the number of points in the plot\n",
    "\n",
    "You can treat the code below as a separate code, it's just created for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "obj_x() got an unexpected keyword argument 'cache_val'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m model \u001b[38;5;241m=\u001b[39m smo_train()\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Prediction in vector\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m train_y \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43malphs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m#print('support vector: {} / {}'\\\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m#.format(len(model['alphs'][model['alphs'] > 1e-5]), len(model['alphs'])))\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Gathers the support vectors (non-zero alpha)\u001b[39;00m\n\u001b[0;32m     40\u001b[0m sv_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[1;32mIn[3], line 22\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(alphs, b, x, new)\u001b[0m\n\u001b[0;32m     19\u001b[0m res: np\u001b[38;5;241m.\u001b[39mfloat64 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x)), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x)):\n\u001b[1;32m---> 22\u001b[0m     fx: np\u001b[38;5;241m.\u001b[39mfloat64 \u001b[38;5;241m=\u001b[39m \u001b[43mobj_x\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnew\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     res[i] \u001b[38;5;241m=\u001b[39m fx\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "\u001b[1;31mTypeError\u001b[0m: obj_x() got an unexpected keyword argument 'cache_val'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "n = 100\n",
    "M = n*2     # Overwrites the above M to make this wor\n",
    "D = 2       # Ensures dimensions are 2\n",
    "\n",
    "def gen_circle(n, center_x=0, center_y=0, radius=1, label=0):\n",
    "    alpha = 2 * np.pi * np.random.rand(n)\n",
    "    r = radius * np.sqrt(np.random.rand(n))\n",
    "    x = r * np.cos(alpha) + center_x\n",
    "    y = r * np.sin(alpha) + center_y\n",
    "    label = np.ones(n) * label\n",
    "    return [x, y, label]\n",
    "\n",
    "C0 = gen_circle(n, center_x=-1, center_y=.5, radius=1, label=1)\n",
    "C1 = gen_circle(n, center_x=1, center_y=-.5, radius=1, label=-1)\n",
    "\n",
    "x0 = np.append(C0[0], C1[0])\n",
    "x1 = np.append(C0[1], C1[1])\n",
    "\n",
    "X = np.c_[x0, x1]\n",
    "Y = np.append(C0[2], C1[2])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_x = scaler.fit_transform(X)\n",
    "\n",
    "# Main function\n",
    "point = train_x\n",
    "target = Y\n",
    "model = smo_train()\n",
    "\n",
    "# Prediction in vector\n",
    "train_y = predict(model['alphs'], model['b'], point, new=False)\n",
    "#print('support vector: {} / {}'\\\n",
    "    #.format(len(model['alphs'][model['alphs'] > 1e-5]), len(model['alphs'])))\n",
    "\n",
    "# Gathers the support vectors (non-zero alpha)\n",
    "sv_threshold = 0\n",
    "sv_idx = []\n",
    "for idx, alpha in enumerate(model['alphs']):\n",
    "    if alpha > sv_threshold:\n",
    "        #print('index = {}, alpha = {:.3f}, predict y={:.3f}'\\\n",
    "            #.format(idx, alpha, train_y[idx]))\n",
    "        \n",
    "        sv_idx.append(idx)\n",
    "\n",
    "# Threshold\n",
    "print(f'bias = {model[\"b\"]}')\n",
    "# Error rate\n",
    "train_y_sign = np.sign(train_y)\n",
    "error_rate = np.mean(train_y_sign != target)\n",
    "print('training data error rate = {:.2f}'.format(error_rate))\n",
    "\n",
    "# Draw the Plot\n",
    "plt.plot(C0[0], C0[1], 'o', markerfacecolor='r', markeredgecolor='None', alpha=0.55)\n",
    "plt.plot(C1[0], C1[1], 'o', markerfacecolor='b', markeredgecolor='None', alpha=0.55)\n",
    "\n",
    "resolution = 50\n",
    "dx = np.linspace(X[:, 0].min(), X[:, 0].max(), resolution)\n",
    "dy = np.linspace(X[:, 1].min(), X[:, 1].max(), resolution)\n",
    "dx, dy = np.meshgrid(dx, dy)\n",
    "plot_x = np.c_[dx.flatten(), dy.flatten()]\n",
    "\n",
    "transformed_plot_x = scaler.transform(plot_x)\n",
    "dz = predict(model['alphs'], model['b'], transformed_plot_x, True)\n",
    "dz = dz.reshape(dx.shape)\n",
    "\n",
    "plt.contour(dx, dy, dz, alpha=1, colors=('b', 'k', 'r'), \\\n",
    "            levels=(-1, 0, 1), linestyles = ('--', '-', '--'))\n",
    "\n",
    "label_cnt = 0\n",
    "for i in sv_idx:\n",
    "    if label_cnt == 0:\n",
    "        plt.scatter(X[i, 0], X[i, 1], marker='*', color='k', \\\n",
    "                    s=120, label='Support vector')\n",
    "        label_cnt += 1\n",
    "        continue\n",
    "\n",
    "    plt.scatter(X[i, 0], X[i, 1], marker='*', color='k', s=120)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
