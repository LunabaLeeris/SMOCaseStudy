{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft margin SVM\n",
    "Soft margin SVM is a branch of SVM (Support Vector Machines) that allows the model to make some level of misclassifications as to make the decision boundary (SOFTER)\n",
    "\n",
    "Specifically, it aims to solve the following dual problem \n",
    "\n",
    "$$\n",
    "max \\space \\sum_{i}\\alpha_i - \\frac{1}{2}\\sum_i \\sum_j y^{(i)}y^{(j)}a_ia_j<x^{(i)}, x^{(j)}> \\\\\n",
    "s.t. \\space C \\ge \\alpha_i \\ge 0 , \\sum_i y^{(i)}\\alpha_i = 0\n",
    "$$\n",
    "\n",
    "With the following KKT conditions\n",
    "\n",
    "$$\n",
    "a_i = 0 \\Rightarrow y^{(i)}(w^Tx^{(i)}+b) \\ge 1 \\\\ \n",
    "a_i = C \\Rightarrow y^{(i)}(w^Tx^{(i)}+b) \\le 1 \\\\ \n",
    "C \\ge a_i \\ge 0 \\Rightarrow y^{(i)}(w^Tx^{(i)}+b) = 1\n",
    "$$\n",
    "\n",
    "Along side with kernel trick, SMO is one of the powerful tools that can do so. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Values\n",
    "- **point** corresponds to the training data $x_i$\n",
    "- **target** corresponds to the training outputs $y_i$\n",
    "- **C** is inversely proportional to the amount of mistakes we can afford. This depends on the scale of the problem. Mostly it’s set from $.01 \\to100$\n",
    "- **tol** is the amount of tolerance we will have for the KKT conditions.\n",
    "- **prog_margin** is the padding we will employ for the calculation of $L$ and $H$ as to not make them equal. This will also serve as our margin in determining whether the two langrange multiplier has made any positive progress.\n",
    "- **clip_padding** is the padding we will apply on the constraint $C \\ge a_i \\ge 0$ where we wil clip $a_i$ to either $C$ or $0$ if it’s within that padding\n",
    "\n",
    "☝🏻 tol, prog_margin and clip_padding are mostly set to $1e^-3$ to $1e^-5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "np.random.seed(690420)\n",
    "\n",
    "M: int = 100\n",
    "D: int = 100\n",
    "point = np.random.normal(size=(M, D), loc=0, scale=10).astype(np.float64)\n",
    "target = np.random.choice([-1, 1], size=(M), replace=True)\n",
    "c: np.float64 = .9\n",
    "tol: np.float64 = 0.0001\n",
    "prog_margin: np.float64 = 0.000001\n",
    "clip_padding: np.float64 = 0.000000001\n",
    "max_ch: int = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Function\n",
    "Function responsible for the kernel trick\n",
    "\n",
    "For gaussian Kernels we use the following \n",
    "$$\n",
    "K(x ,z ) = exp(-\\frac{||x-z||^2}{2\\sigma})\n",
    "$$\n",
    "\n",
    "This can be sped up from the fact that $||x-z||^2 = x * x - 2x*z + z *z $\n",
    "\n",
    "We can cache the dot product of vector to itself. We can also store the dot product of every 2 possible pair! but this may take a lot of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_dot_cache: np.float32 = np.matmul(point, np.transpose(point))\n",
    "sigma: int = 10\n",
    "\n",
    "def kernel_gaussian(x: np.float32, z: np.array, cache_x: int=-1, cache_z: int=-1) -> float:\n",
    "    '''\n",
    "    cache_x and cache_z allows us to determine whether x and z are stored in self_dot_cache.\n",
    "    This helps us calculate the dot product of a vector with itself easier\n",
    "    '''\n",
    "    dot_xx: float = self_dot_cache[cache_x, cache_x] if cache_x > -1 else np.dot(x, x)\n",
    "    dot_zz: float = self_dot_cache[cache_z, cache_z] if cache_z > -1 else np.dot(z, z)\n",
    "    dot_xz: float = self_dot_cache[cache_x, cache_z] if (cache_z > -1 and cache_x > -1) else np.dot(x, z)\n",
    "    return math.e**((-1/(2 * sigma)) * (dot_xx - 2*dot_xz + dot_zz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_svm_err(x: int, alphs: np.float64, err_cache: np.float64, b: np.float64) -> np.float64:\n",
    "    fx: np.float64 = obj_x(point[x], alphs, b, cache_val=x)\n",
    "    err_cache[x] = fx - target[x]\n",
    "\n",
    "    return err_cache[x]     \n",
    "\n",
    "def obj_x(val: np.float64, alphs: np.float64, b: np.float64, cache_val: int=-1) -> np.float64:\n",
    "    fx: np.float64 = -b\n",
    "    alphs_zero: np.float64 = np.where(alphs != 0)[0]\n",
    "    \n",
    "    for i in alphs_zero:\n",
    "        fx += alphs[i] *target[i] * kernel_gaussian(point[i], val, cache_x=i, cache_z=cache_val)\n",
    "        \n",
    "    return fx\n",
    "\n",
    "def accuracy(alphs: np.float64, b: np.float64) -> np.float64:\n",
    "    correct: int = 0\n",
    "    for i in range(M):\n",
    "        fx: np.float64 = obj_x(point[i], alphs, b, cache_val=i)\n",
    "        h : int = 1 if fx >= 0 else -1\n",
    "        correct += (target[i] == h)\n",
    "    \n",
    "    return correct / M\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Function\n",
    "Lastly the function that takes a coordinate ascent step given the two multipliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_step(i1: int, i2: int, alphs: np.float64, non_bound_alphs: np.int64, err_cache: np.float64, B: np.float64, log: bool=False) -> bool:\n",
    "    print(f\"Taking step for {i1} and {i2}\")\n",
    "    if (i1 == i2):\n",
    "        print(\"Positions equal\")\n",
    "        return 0\n",
    "    \n",
    "    non_bound: bool = alphs[i1] != 0 and alphs[i1] != c\n",
    "    b: np.float64 = B[0]\n",
    "\n",
    "    E1: np.float64 = err_cache[i1] if non_bound else compute_svm_err(i1, alphs, err_cache, b)\n",
    "    E2: np.float64 = err_cache[i2] \n",
    "    y1: int = target[i1]\n",
    "    y2: int = target[i2]\n",
    "    alph1: np.float64 = alphs[i1]\n",
    "    alph2: np.float64 = alphs[i2]\n",
    "    s: int = y1 * y2\n",
    "\n",
    "    # Computation for L and H\n",
    "    if (y1 == y2):\n",
    "        L: np.float64 = max(0, alph1 + alph2 - c)\n",
    "        H: np.float64 = min(c, alph1 + alph2)\n",
    "    else:\n",
    "        L: np.float64 = max(0, alph2 - alph1)\n",
    "        H: np.float64 = min(c, c + alph2 - alph1)\n",
    "\n",
    "    if (L == H):\n",
    "        print(\"L and H equal\")\n",
    "        return 0\n",
    "    \n",
    "    K11: np.float64 = kernel_gaussian(point[i1], point[i1], cache_x=i1, cache_z=i1)\n",
    "    K22: np.float64 = kernel_gaussian(point[i2], point[i2], cache_x=i2, cache_z=i2)\n",
    "    K12: np.float64 = kernel_gaussian(point[i1], point[i2], cache_x=i1, cache_z=i2)\n",
    "    eta: np.float64 = (2*K12)- K11 - K22\n",
    "\n",
    "    if (eta < 0):\n",
    "        alph2_new: np.float64 = alph2 - (y2*(E1 - E2)/eta)\n",
    "        if (alph2_new < L):\n",
    "            alph2_new = L\n",
    "        elif (alph2_new > H):\n",
    "            alph2_new = H\n",
    "    else:\n",
    "        f1: np.float64 = y1 * (E1 + B) - alph1 * K11 - s * alph2 * K12\n",
    "        f2: np.float64 = y2 * (E2 + B) - s * alph1 * K12 - alph2 * K22\n",
    "        L1: np.float64 = alph1 + s * (alph2 - L)\n",
    "        H1: np.float64 = alph1 + s * (alph2 - H)\n",
    "        Lobj: np.float64 = L1 * f1 + L * f2 + (0.5 * (L1 ** 2) * K11) + (0.5 * (L ** 2) * K22) + (s * L * L1 * K12)\n",
    "        Hobj: np.float64 = H1 * f1 + H * f2 + (0.5 * (H1 ** 2) * K11) + (0.5 * (H ** 2) * K22) + (s * H * H1 * K12)\n",
    "\n",
    "        if (Lobj < Hobj - prog_margin):\n",
    "            alph2_new: np.float64 = H\n",
    "        elif (Lobj > Hobj + prog_margin):\n",
    "            alph2_new: np.float64 = L    \n",
    "        else:\n",
    "            alph2_new: np.float64 = alph2\n",
    "\n",
    "    # clip\n",
    "    if (alph2_new < clip_padding):\n",
    "        alph2_new = 0\n",
    "    elif (alph2_new > c - clip_padding):\n",
    "        alph2_new = c\n",
    "        \n",
    "    if (abs(alph2_new - alph2) < (prog_margin * (alph2_new + alph2 + prog_margin))):\n",
    "        print(\"Bad progress\")\n",
    "        return 0\n",
    "    \n",
    "    alph1_new: np.float64 = alph1 + (s*(alph2 - alph2_new))\n",
    "\n",
    "    # update tresholds\n",
    "    b1: np.float64 = E1 + y1*K11*(alph1_new - alph1) + y2*K12*(alph2_new - alph2) + b\n",
    "    b2: np.float64 = E2 + y1*K12*(alph1_new - alph1) + y2*K22*(alph2_new - alph2) + b\n",
    "    B[0] = (b1/2) + (b2/2)\n",
    "    \n",
    "    # update err_cache\n",
    "    err_cache[i1], err_cache[i2] = 0, 0\n",
    "    for i in non_bound_alphs:\n",
    "        if (i == i1 or i == i2):\n",
    "            continue\n",
    "        \n",
    "        K1k: np.float64 = kernel_gaussian(point[i], point[i1], cache_x=i, cache_z=i1)\n",
    "        K2k: np.float64 = kernel_gaussian(point[i], point[i2], cache_x=i, cache_z=i2)\n",
    "\n",
    "        err_cache[i] = err_cache[i] + y1*K1k*(alph1_new - alph1) + y2*K2k*(alph2_new - alph2) + b - B[0]\n",
    "    \n",
    "    # update alphs\n",
    "    alphs[i1], alphs[i2] = alph1_new, alph2_new\n",
    "    \n",
    "    if (log):\n",
    "        print(f\"======= Step successful for {i1} and {i2} =======\")\n",
    "        print(f\"a1: {alph1} -> {alph1_new} | a2: {alph2} -> {alph2_new}\")\n",
    "        print(f\"b: {b} -> {B[0]}\")\n",
    "        print(f\"err_cache: {err_cache}\")\n",
    "        print(\"==================================================\")\n",
    "\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Alpha Function\n",
    "The second function which is responsible for checking the first langrange multiplier that is chosen and responsible for picking the next langrange multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_a(i2: int, alphs: np.array, non_bound: bool, non_bound_alphs: np.int64, err_cache: np.array, B: np.float64) -> bool:\n",
    "    non_b_len: int = len(non_bound_alphs)\n",
    "    b: np.float64 = B[0]\n",
    "    E2: np.float64 = err_cache[i2] if non_bound else compute_svm_err(i2, alphs, err_cache, b)\n",
    "    r2: np.float64 = E2 * target[i2]\n",
    "    alph2: np.float64 = alphs[i2]\n",
    "\n",
    "    if ((r2 < -tol and alph2 < c) or (r2 > tol and alph2 > 0)):\n",
    "        print(\"choosing second multipier\")\n",
    "        if (non_b_len > 1):\n",
    "            # Second heuristic using optimal err for step estimation\n",
    "            positive: bool = (err_cache[i2] > 0)\n",
    "            i1: int = i2\n",
    "\n",
    "            for i in non_bound_alphs:\n",
    "                if (i == i2):\n",
    "                    continue\n",
    "\n",
    "                if (i1 == i2):\n",
    "                    i1 = i\n",
    "                elif (positive and err_cache[i] < err_cache[i1]):\n",
    "                    i1 = i\n",
    "                elif (not positive and err_cache[i] > err_cache[i1]):\n",
    "                    i1 = i\n",
    "\n",
    "            if (take_step(i1, i2, alphs, non_bound_alphs, err_cache, B)):\n",
    "                return 1\n",
    "\n",
    "        # take non-bound alps\n",
    "        if non_b_len > 0:\n",
    "            start: int = np.random.randint(size=(1), low=0, high=non_b_len)[0]\n",
    "            for i in range(non_b_len):\n",
    "                pos: int = (start + i)%non_b_len\n",
    "                if (take_step(non_bound_alphs[pos], i2, alphs, non_bound_alphs, err_cache, B)):\n",
    "                    return 1\n",
    "        \n",
    "        # loop through entire training set\n",
    "        start: int = np.random.randint(size=(1), low=0, high=M)[0]\n",
    "        for i in range(M):\n",
    "            if (take_step((start + i)%M, i2, alphs, non_bound_alphs, err_cache, B)):\n",
    "                return 1\n",
    "    \n",
    "    print(\"already satisfy kkt conditions\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Function\n",
    "The train function which is responsible for picking the first langrange multiplier from a set of langrange multipliers. It is also responsible for initializing the important variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smo_train() -> dict:\n",
    "    alphs: np.float64 = np.zeros(shape=(M), dtype=np.float64)\n",
    "    err_cache: np.float64 = np.zeros(shape=(M), dtype=np.float64)\n",
    "    B: np.float64 = np.array([0.], dtype=np.float64)\n",
    "\n",
    "    examine_all: bool = True\n",
    "    num_changed: int = 0\n",
    "    total_changed: int = 0\n",
    "\n",
    "    while num_changed > 0 or examine_all:\n",
    "        print(\"choosing first multiplier\")\n",
    "        num_changed = 0\n",
    "\n",
    "        non_bound_alphs = np.where((alphs != 0) & (alphs != c))[0]\n",
    "\n",
    "        if examine_all:\n",
    "            for i in range(M):\n",
    "                num_changed += examine_a(i, alphs, False, non_bound_alphs, err_cache, B)\n",
    "        else:\n",
    "            for i in non_bound_alphs:\n",
    "                num_changed += examine_a(i, alphs, True, non_bound_alphs, err_cache, B)\n",
    "\n",
    "        if examine_all:\n",
    "            examine_all = False\n",
    "        elif num_changed == 0:\n",
    "            examine_all = True\n",
    "\n",
    "        total_changed += num_changed\n",
    "        if total_changed > max_ch:\n",
    "            return {\"alphs\": alphs, \"err_cache\": err_cache, \"b\": B[0]}\n",
    "\n",
    "        print(\"total iterations: \", total_changed)\n",
    "\n",
    "    print(err_cache)\n",
    "    return {\"alphs\": alphs, \"err_cache\": err_cache, \"b\": B[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "You can test the result here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "'''\n",
    "x = [(0.1 + (i * 0.1)) for i in range(1, 10)]\n",
    "y = []\n",
    "\n",
    "for i in range(len(x)):\n",
    "    c = x[i]\n",
    "    res = smo_train()\n",
    "    y.append(accuracy(res[\"alphs\"], res[\"b\"]))\n",
    "    \n",
    "plt.xlabel(\"c\")\n",
    "plt.ylabel(\"average distance\")\n",
    "plt.scatter(x, y)\n",
    "'''\n",
    "\n",
    "res = smo_train()\n",
    "print(accuracy(res[\"alphs\"], res[\"b\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(x, [abs(i) for i in y])\n",
    "'''\n",
    "alphs: np.float64 = np.zeros(shape=(M), dtype=np.float64)\n",
    "print(accuracy(alphs, 0))\n",
    "print(res[\"b\"])\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
