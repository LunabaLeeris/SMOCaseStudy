{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft margin SVM\n",
    "Soft margin SVM is a branch of SVM (Support Vector Machines) that allows the model to make some level of misclassifications as to make the decision boundary (SOFTER)\n",
    "\n",
    "Specifically, it aims to solve the following dual problem \n",
    "\n",
    "$$\n",
    "max \\space \\sum_{i}\\alpha_i - \\frac{1}{2}\\sum_i \\sum_j y^{(i)}y^{(j)}a_ia_j<x^{(i)}, x^{(j)}> \\\\\n",
    "s.t. \\space C \\ge \\alpha_i \\ge 0 , \\sum_i y^{(i)}\\alpha_i = 0\n",
    "$$\n",
    "\n",
    "With the following KKT conditions\n",
    "\n",
    "$$\n",
    "a_i = 0 \\Rightarrow y^{(i)}(w^Tx^{(i)}+b) \\ge 1 \\\\ \n",
    "a_i = C \\Rightarrow y^{(i)}(w^Tx^{(i)}+b) \\le 1 \\\\ \n",
    "C \\ge a_i \\ge 0 \\Rightarrow y^{(i)}(w^Tx^{(i)}+b) = 1\n",
    "$$\n",
    "\n",
    "Along side with kernel trick, SMO is one of the powerful tools that can do so. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Values\n",
    "- **point** corresponds to the training data $x_i$\n",
    "- **target** corresponds to the training outputs $y_i$\n",
    "- **C** is proportional to the amount of mistakes we can afford. This depends on the scale of the problem. Mostly itâ€™s set from $.01 \\to100$\n",
    "- **tol** is the amount of tolerance we will have for the KKT conditions.\n",
    "- **prog_margin** is the padding we will employ for the calculation of $L$ and $H$ as to not make them equal. This will also serve as our margin in determining whether the two langrange multiplier has made any positive progress.\n",
    "- **clip_padding** is the padding we will apply on the constraint $C \\ge a_i \\ge 0$ where we wil clip $a_i$ to either $C$ or $0$ if itâ€™s within that padding\n",
    "\n",
    "â˜ðŸ» tol, prog_margin and clip_padding are mostly set to $1e^-3$ to $1e^-5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(69420)\n",
    "\n",
    "M, D = 50, 10\n",
    "point = np.random.normal(size=(M, D), loc=0, scale=1).astype(np.float32)\n",
    "target = np.random.randint(size=(1, M), low=0, high=2)\n",
    "c = .1\n",
    "tol, prog_margin, clip_paddin = .001, .001, .001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Function\n",
    "Function responsible for the kernel trick\n",
    "\n",
    "For gaussian Kernels we use the following \n",
    "$$\n",
    "K(x ,z ) = exp(-\\frac{||x-z||^2}{2\\sigma})\n",
    "$$\n",
    "\n",
    "This can be sped up from the fact that $||x-z||^2 = x * x - 2x*z + z *z $\n",
    "\n",
    "We can cache the dot product of vector to itself. We can also store the dot product of every 2 possible pair! but this may take a lot of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_dot_cache = np.zeros(shape=(M), dtype=np.float32)\n",
    "sigma = 1\n",
    "\n",
    "# initialize self_dot_cache\n",
    "def initialize_sdc(): \n",
    "    for i in range(M):\n",
    "        self_dot_cache[i] = np.dot(point[i], point[i])\n",
    "\n",
    "def kernel_gaussian(x, z):\n",
    "    return np.e((-1/(2 * sigma)) * (self_dot_cache[x] - 2 * np.dot(x, z) + self_dot_cache[z]))\n",
    "\n",
    "initialize_sdc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Function\n",
    "The train function which is responsible for picking the first langrange multiplier from a set of langrange multipliers. It is also responsible for initializing the important variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smo_train():\n",
    "    alphs = np.zeros(shape=(1, M), dtype=np.float32)\n",
    "    w = np.zeros(shape=(1, D), dtype=np.float32)\n",
    "    err_cache = np.zeros(shape=(1, M), dtype=np.float32)\n",
    "    b = 0\n",
    "\n",
    "    examine_all = True\n",
    "    num_changed = False\n",
    "\n",
    "    while (num_changed > 0 or examine_all):\n",
    "        if (examine_all):\n",
    "            for i in range(M):\n",
    "                num_changed += examine_a(i, alphs, err_cache, b)\n",
    "        \n",
    "        else:\n",
    "            for i in range(M):\n",
    "                if (alphs[i] != 0 or c):\n",
    "                    num_changed += examine_a(i, alphs, err_cache,)\n",
    "\n",
    "        if (examine_all):\n",
    "            examine_all = False\n",
    "        \n",
    "        elif (num_changed == 0):\n",
    "            examine_all = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Cache Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Alpha Function\n",
    "The second function which is responsible for checking the first langrange multiplier that is chosen and responsible for picking the next langrange multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_a(i2, alphs, err_cache, b):\n",
    "    non_bound = alphs[i2] != 0 or c\n",
    "    E2 = err_cache(i2) if non_bound else compute_svm_err(i2, alphs, err_cache, b)\n",
    "    r2 = E2 * target[i2]\n",
    "    alph2 = alphs[i2]\n",
    "\n",
    "    if ((r2 < -tol and alph2 < c) or (r2 > tol and alph2 > 0)):\n",
    "        i1 = choose_second(i2, err_cache)\n",
    "    \n",
    "def choose_second(first, err_cache):\n",
    "    pos = err_cache[first] >= 0\n",
    "    best = first\n",
    "\n",
    "    for i in range(M):\n",
    "        if (i == first):\n",
    "            continue\n",
    "\n",
    "        if (best == first):\n",
    "            best = i\n",
    "        \n",
    "        best = min(err_cache[i], err_cache[best]) if pos else max(err_cache[i], err_cache[best])\n",
    "\n",
    "    return best\n",
    "\n",
    "def compute_svm_err(x, alphs, err_cache, b):\n",
    "    fx = obj_func(x, alphs, b)\n",
    "    err_cache[x] = fx - b\n",
    "    return err_cache[x]\n",
    "\n",
    "def obj_func(x, alphs, b):\n",
    "    fx = 0\n",
    "    for i in range(M):\n",
    "        fx += alphs[i] * target[i] * kernel_gaussian(i, x)\n",
    "\n",
    "    return fx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Function\n",
    "Lastly the function that takes a coordinate ascent step given the two multipliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_step(i1, i2, alphs, err_cache, b):\n",
    "    if (i1 == i2):\n",
    "        return 0\n",
    "    \n",
    "    non_bound = alphs[i1] != 0 or c\n",
    "    E1 = err_cache(i1) if non_bound else compute_svm_err(i1)\n",
    "    E2 = err_cache(i2)\n",
    "    y2 = target[i2]\n",
    "    y1 = target[i1]\n",
    "    alph1, alph2 = alphs[i1], alphs[i2]\n",
    "    s = y1 * y2\n",
    "\n",
    "    # Computation for L and H\n",
    "    if (y1 == y2):\n",
    "        L, H = max(0, alph1 + alph2 - c), min(c, alph1 + alph2)\n",
    "    else:\n",
    "        L, H = max(0, alph2 - alph1), min(c, c + alph2 - alph1)\n",
    "\n",
    "    if (L == H):\n",
    "        return 0\n",
    "    \n",
    "    K11 = self_dot_cache(i1)\n",
    "    K22 = self_dot_cache(i2)\n",
    "    K12 = kernel_gaussian(i1, i2)\n",
    "    \n",
    "    eta = 2*K12 - K11 - K22\n",
    "\n",
    "    if (eta < 0):\n",
    "        alph2_new = alph2 - y2*abs(E2 - E2)/eta\n",
    "        if (alph2_new < L):\n",
    "            alph2_new = L\n",
    "        elif (alph2_new > H):\n",
    "            alph2_new = H\n",
    "    else:\n",
    "        Lobj = obj_func(L, obj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
